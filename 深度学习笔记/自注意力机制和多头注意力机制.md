## 1.Self-Attention
    小结:自注意力机制主要关注的是三个值Q、K、V。这三个值是由输入向量/矩阵x分别进行不同的线性变换得到的,即乘相应的矩阵Wq、Wk、Wv。这三个矩阵随机初始化然后在整个训练过程中进行权重更新。这也是为什叫做自注意力机制的原因。

### 1.1步骤
1.生成三个值Q、K、V。这三个向量是通过输入的词向量x和三个矩阵做点乘得到的。这三个矩阵的权重先随机初始化，之后会在训练过程中调整。
2.使用Q和K相乘得到score，这score的值决定了我们需要放多少注意力在相应的其它的输入上（即进行相似度计算）。
![alt text](https://i-blog.csdnimg.cn/blog_migrate/d6dca7ec562f69c90e0134b7a13bf9b1.png)
3.进行除以dk和softmax处理（softmax能转化成概率）
![alt text](https://pica.zhimg.com/v2-4452fdaaa04686aa270010f57f4db2aa_r.jpg)
4.用每个单词得到的soft score和得到的不同输入的值向量V相乘得到multiplication。在这一步，我们保持那些需要注意的单词的值的完整性。并且，冲淡了那些与当前单词关联性不强的单词

5.把上一步中得到的每一个向量multiplication对应元素相加相加得到最后输出addition
![alt text](https://i-blog.csdnimg.cn/blog_migrate/404863933acbb890777f3ed9aac294bb.png)
![alt text](https://i-blog.csdnimg.cn/blog_migrate/3fabea30afe362c864b261a7e4de1772.png)

6.输入有多少个向量就对这些向量依次重复多少次上述操作

## 2.多头注意力机制
多头注意力机制的目的是为了从多个维度捕捉提取更多的特征，从多个“头”得到不同的Self-Attention Score，提高模型表现。

首先放一张论文原文中的多头注意力机制的架构（Multi-Head Attention），可以看到（V,K,Q）三个矩阵通过h个线性变换（Linear），分别得到h组（V,K,Q）矩阵，每一组（V,K,Q）经过Attention计算，得到h个Attention Score并进行拼接（Concat），最后通过一个线性变换得到输出，其维度与输入词向量的维度一致，其中h就是多头注意力机制的“头数”。

![alt text](https://i-blog.csdnimg.cn/blog_migrate/5fd02f549e1d4e0dc33f8ef36202011d.png)

    多头注意力机制就是自注意力机制的套娃，就是对于每一组产生的Q K V值之后设定一个头h，这个h的大小就是后续变换矩阵的数量，对这一组的Q K V进行一次变换得到h组Q K V 然后最每一个Q K V进行自注意力机制的重复操作最后把得到的addition输出相加得到最终的addition就完成了多头注意力机制了。