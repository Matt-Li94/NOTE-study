# 一些常用的重要概念
## 1.迁移学习
**迁移学习理解：一般用在已有一个预训练好的模型，并将其应用到现有任务上的过程**


源领域和目标领域的任务不同，但数据分布相似。通过在源领域上训练模型，然后在目标领域上微调。通俗讲就是训练在一个数据集上，然而应用是在另外一个数据集上，一般有以下两种方式

（1） 基于特征的方式：将预训练模型的输出或者是中间隐藏层的输出作为 特征直接加入到目标任务的学习模型中．目标任务的学习模型可以是一般的浅 层分类器（比如全连接层等）或一个新的神经网络模型．

（2） 精调的方式：在目标任务上复用预训练模型的部分组件，并对其参数 进行精调（Fine-Tuning）．

微调的策略（实验中体现）
全模型微调：对整个模型进行微调，适用于源任务和目标任务相似的情况。

部分微调：只微调模型的最后几层，适用于源任务和目标任务差异较大的情况。

冻结层：在微调过程中，可以选择冻结某些层的权重，只训练特定的层，以保持预训练模型的特征提取能力。

eg：b站小土堆使用的vgg16篇章，直接修改vgg16中的一些网络层来适配自己的分类任务，如下图所示：

![alt text](https://i-blog.csdnimg.cn/blog_migrate/ce9540e5341406eeefe0de35e410807a.png)

实操代码如下：

```python
vgg16_true.classifier.add_module(name="7",module=torch.nn.Linear(1000,10)) #直接添加自己想要的线形层
vgg16_false.classifier[6] = torch.nn.Linear(4096,10) #直接修改某一层
```

有个问腿：修改了之后是不是要重新训练网络？

## 2.残差网络ResNet
### 2.1残差网络ResNet的原理
（这个东西原理不是很重要吧，知道怎么用我觉得就可以了

首先，我们把网络层看成是映射函数：

(1)在传统的前馈网络中，网络中堆叠的层可以将输入x映射为F(x)，这一整体网络的输出为H(x)，F(x)=H(x)。但是对于恒等映射函数f(x)=x，即网络的输入与输出相等，直接让这样的层去拟合这样的恒等映射函数会很困难，不过f(x)=0还是比较容易训练拟合的。

(2)所以我们可以让输出H(x)=F(x)+x，这样如果整体网络H(x)需要是恒等映射，只需要把堆叠层拟合成F(x)=0即可。

拟合H(x)-x与（1）中那样直接拟合F(x)相比更简单，这和提到的恒等映射函数不好优化这种说法是一致的。具体操作如下图所示：

![alt text](https://i-blog.csdnimg.cn/blog_migrate/1665f81adf43491dd09598d1990ea073.png)

对于上述的理解：一个网络直接对于进行输入x，输出f（x）= x的恒等拟合是很困难的，所以我们把x加到输出f（x）中，那么对于整个网络来说我们想要获得恒等拟合只需要把任务转化为让网络输出f（x）=0 而不是f（x）=x即可。这个任务对于网络就比较简单了。


### 2.2残差网络ResNet能解决的问题
随着神经网络层数的增加，深度网络面临两个主要问题：
```python
1.梯度消失和梯度爆炸：在反向传播中，梯度会随着层数的增加逐层变小或变大，导致前几层的权重更新非常缓慢或更新过大，模型难以有效训练。

2.退化问题：在非常深的网络中，增加更多的层有时反而会导致模型的训练误差增大，而不是进一步减少。理想情况下，增加更多的层应该至少不会使性能变差，但实际上在深度网络中，随着层数增加，网络的表示能力可能反而下降。
```
ResNet内部使用多个具有残差连接的残差块来解决梯度消失或梯度爆炸问题，并使得网络可以向更深层发展，同时有效避免了退化问题

### 2.3ResNet 在实际中的应用
ResNet 在许多实际任务中表现优异，尤其是在图像处理任务上，它成为了许多深度学习模型的基础组件：
```python
1.图像分类：ResNet 广泛应用于图像分类任务中，特别是在 ImageNet 这种大规模数据集上的表现非常突出。
2.目标检测和分割：ResNet 被广泛用于目标检测和语义分割的任务中，例如 Faster R-CNN、Mask R-CNN 等模型都使用 ResNet 作为特征提取器。
3.自然语言处理：虽然 ResNet 主要用于图像处理，但它的残差思想也被迁移到自然语言处理（NLP）领域，用于构建深层语言模型。
```